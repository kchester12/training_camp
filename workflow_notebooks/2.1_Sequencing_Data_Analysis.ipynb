{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequencing data analysis\n",
    "\n",
    "### IMPORTANT: Please make sure that your are using the bash kernel to run this notebook.\n",
    "### IMPORTANT: Run the command below to git pull and make sure you are running the latest code!! ###\n",
    "#### (Do this at the beginning of every session) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##UPDATES \n",
    "1. single helper script to run the pipeline: (ATAC-seq pipeline)\n",
    "2. students will have to generate count file from tagAlign & bed \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No local changes to save\r\n",
      "remote: Counting objects: 48, done.        \r\n",
      "remote: Compressing objects:   2% (1/44)           \r",
      "remote: Compressing objects:   4% (2/44)           \r",
      "remote: Compressing objects:   6% (3/44)           \r",
      "remote: Compressing objects:   9% (4/44)           \r",
      "remote: Compressing objects:  11% (5/44)           \r",
      "remote: Compressing objects:  13% (6/44)           \r",
      "remote: Compressing objects:  15% (7/44)           \r",
      "remote: Compressing objects:  18% (8/44)           \r",
      "remote: Compressing objects:  20% (9/44)           \r",
      "remote: Compressing objects:  22% (10/44)           \r",
      "remote: Compressing objects:  25% (11/44)           \r",
      "remote: Compressing objects:  27% (12/44)           \r",
      "remote: Compressing objects:  29% (13/44)           \r",
      "remote: Compressing objects:  31% (14/44)           \r",
      "remote: Compressing objects:  34% (15/44)           \r",
      "remote: Compressing objects:  36% (16/44)           \r",
      "remote: Compressing objects:  38% (17/44)           \r",
      "remote: Compressing objects:  40% (18/44)           \r",
      "remote: Compressing objects:  43% (19/44)           \r",
      "remote: Compressing objects:  45% (20/44)           \r",
      "remote: Compressing objects:  47% (21/44)           \r",
      "remote: Compressing objects:  50% (22/44)           \r",
      "remote: Compressing objects:  52% (23/44)           \r",
      "remote: Compressing objects:  54% (24/44)           \r",
      "remote: Compressing objects:  56% (25/44)           \r",
      "remote: Compressing objects:  59% (26/44)           \r",
      "remote: Compressing objects:  61% (27/44)           \r",
      "remote: Compressing objects:  63% (28/44)           \r",
      "remote: Compressing objects:  65% (29/44)           \r",
      "remote: Compressing objects:  68% (30/44)           \r",
      "remote: Compressing objects:  70% (31/44)           \r",
      "remote: Compressing objects:  72% (32/44)           \r",
      "remote: Compressing objects:  75% (33/44)           \r",
      "remote: Compressing objects:  77% (34/44)           \r",
      "remote: Compressing objects:  79% (35/44)           \r",
      "remote: Compressing objects:  81% (36/44)           \r",
      "remote: Compressing objects:  84% (37/44)           \r",
      "remote: Compressing objects:  86% (38/44)           \r",
      "remote: Compressing objects:  88% (39/44)           \r",
      "remote: Compressing objects:  90% (40/44)           \r",
      "remote: Compressing objects:  93% (41/44)           \r",
      "remote: Compressing objects:  95% (42/44)           \r",
      "remote: Compressing objects:  97% (43/44)           \r",
      "remote: Compressing objects: 100% (44/44)           \r",
      "remote: Compressing objects: 100% (44/44), done.        \r\n",
      "Unpacking objects:   2% (1/48)   \r",
      "Unpacking objects:   4% (2/48)   \r",
      "error: insufficient permission for adding an object to repository database .git/objects\r\n",
      "fatal: failed to write object\r\n",
      "fatal: unpack-objects failed\r\n"
     ]
    }
   ],
   "source": [
    "cd /srv/scratch/training_camp/tc2017/`whoami`/src/training_camp\n",
    "git stash \n",
    "git pull "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook covers analysis of DNA sequencing data from raw files to processed signals.\n",
    "\n",
    "Although this analysis is for ATAC-seq data, many of the steps (especially the first section) are the same for other types of DNA sequencing experiments.\n",
    "\n",
    "We'll be doing the analysis in Bash, which is the standard language for UNIX command-line scripting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps in the analysis pipeline that are covered in this notebook are indicated below:\n",
    "![Sequencing Data Analysis 1](part1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting up the data\n",
    "\n",
    "We start with raw `.fastq.gz` files, which are provided by the sequencing instrument. For each DNA molecule (read) that was sequenced, they provide the nucleotide sequence, and information about the quality of the signal of that nucleotide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "### Set up variables storing the location of our data\n",
    "### The proper way to load your variables is with the ~/.bashrc command, but this is very slow in iPython \n",
    "export SUNETID=\"$(whoami)\"\n",
    "export WORK_DIR=\"/srv/scratch/training_camp/tc2017/${SUNETID}\"\n",
    "export DATA_DIR=\"${WORK_DIR}/data\"\n",
    "export FASTQ_DIR=\"${DATA_DIR}/fastq/\"\n",
    "export SRC_DIR=\"${WORK_DIR}/src/training_camp/src/\"\n",
    "export ANALYSIS_DIR=\"${WORK_DIR}/analysis/\"\n",
    "export YEAST_DIR=\"/srv/scratch/training_camp/saccer3/seq\"\n",
    "export YEAST_INDEX=\"/srv/scratch/training_camp/saccer3/bowtie2_index/saccer3\"\n",
    "export YEAST_CHR=\"/srv/scratch/training_camp/saccer3/sacCer3.chrom.sizes\"\n",
    "export TMP=\"${WORK_DIR}/tmp\"\n",
    "export TEMP=$TMP \n",
    "export TMPDIR=$TMP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check exactly which fastqs we have:\n",
    "\n",
    "(recall that the `ls` command lists the contents of a directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WT-SCD-0_6MNaCl-Rep1_R1_001.fastq.gz\tcln3-SCD-Rep1_R1_001.fastq.gz\r\n",
      "WT-SCD-0_6MNaCl-Rep1_R2_001.fastq.gz\tcln3-SCD-Rep1_R2_001.fastq.gz\r\n",
      "WT-SCD-0_6MNaCl-Rep2_R1_001.fastq.gz\tcln3-SCD-Rep2_R1_001.fastq.gz\r\n",
      "WT-SCD-0_6MNaCl-Rep2_R2_001.fastq.gz\tcln3-SCD-Rep2_R2_001.fastq.gz\r\n",
      "WT-SCD-Rep1_R1_001.fastq.gz\t\tcln3-SCE-0_6MNaCl-Rep1_R1_001.fastq.gz\r\n",
      "WT-SCD-Rep1_R2_001.fastq.gz\t\tcln3-SCE-0_6MNaCl-Rep1_R2_001.fastq.gz\r\n",
      "WT-SCD-Rep2_R1_001.fastq.gz\t\tcln3-SCE-0_6MNaCl-Rep2_R1_001.fastq.gz\r\n",
      "WT-SCD-Rep2_R2_001.fastq.gz\t\tcln3-SCE-0_6MNaCl-Rep2_R2_001.fastq.gz\r\n",
      "WT-SCE-0_6MNaCl-Rep1_R1_001.fastq.gz\tcln3-SCE-Rep1_R1_001.fastq.gz\r\n",
      "WT-SCE-0_6MNaCl-Rep1_R2_001.fastq.gz\tcln3-SCE-Rep1_R2_001.fastq.gz\r\n",
      "WT-SCE-0_6MNaCl-Rep2_R1_001.fastq.gz\tcln3-SCE-Rep2_R1_001.fastq.gz\r\n",
      "WT-SCE-0_6MNaCl-Rep2_R2_001.fastq.gz\tcln3-SCE-Rep2_R2_001.fastq.gz\r\n",
      "WT-SCE-Rep1_R1_001.fastq.gz\t\twhi5-SCE-Rep1_R1_001.fastq.gz\r\n",
      "WT-SCE-Rep1_R2_001.fastq.gz\t\twhi5-SCE-Rep1_R2_001.fastq.gz\r\n",
      "WT-SCE-Rep2_R1_001.fastq.gz\t\twhi5-SCE-Rep2_R1_001.fastq.gz\r\n",
      "WT-SCE-Rep2_R2_001.fastq.gz\t\twhi5-SCE-Rep2_R2_001.fastq.gz\r\n",
      "cln3-SCD-0_6MNaCl-Rep1_R1_001.fastq.gz\twhi5-cln3-SCE-Rep1_R1_001.fastq.gz\r\n",
      "cln3-SCD-0_6MNaCl-Rep1_R2_001.fastq.gz\twhi5-cln3-SCE-Rep1_R2_001.fastq.gz\r\n",
      "cln3-SCD-0_6MNaCl-Rep2_R1_001.fastq.gz\twhi5-cln3-SCE-Rep2_R1_001.fastq.gz\r\n",
      "cln3-SCD-0_6MNaCl-Rep2_R2_001.fastq.gz\twhi5-cln3-SCE-Rep2_R2_001.fastq.gz\r\n"
     ]
    }
   ],
   "source": [
    "ls $FASTQ_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As a sanity check, we can also look at the size and last edited time of some of the fastqs by addind `-lrth` to the `ls` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 5.4G\r\n",
      "-rwxrwxr-x 1 user1 user1  32M Sep 21 17:07 WT-SCD-Rep1_R1_001.fastq.gz\r\n",
      "-rwxrwxr-x 1 user1 user1  28M Sep 21 17:07 WT-SCD-Rep1_R2_001.fastq.gz\r\n",
      "-rwxrwxr-x 1 user1 user1 168M Sep 21 17:07 WT-SCD-Rep2_R1_001.fastq.gz\r\n",
      "-rwxrwxr-x 1 user1 user1 157M Sep 21 17:07 WT-SCD-Rep2_R2_001.fastq.gz\r\n",
      "-rwxrwxr-x 1 user1 user1 203M Sep 21 17:07 WT-SCE-0_6MNaCl-Rep1_R1_001.fastq.gz\r\n",
      "-rwxrwxr-x 1 user1 user1 183M Sep 21 17:07 WT-SCE-0_6MNaCl-Rep1_R2_001.fastq.gz\r\n",
      "-rwxrwxr-x 1 user1 user1 212M Sep 21 17:07 WT-SCE-0_6MNaCl-Rep2_R1_001.fastq.gz\r\n",
      "-rwxrwxr-x 1 user1 user1 199M Sep 21 17:07 WT-SCE-0_6MNaCl-Rep2_R2_001.fastq.gz\r\n",
      "-rwxrwxr-x 1 user1 user1 9.7M Sep 21 17:07 WT-SCE-Rep1_R1_001.fastq.gz\r\n"
     ]
    }
   ],
   "source": [
    "ls -lrth $FASTQ_DIR | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also inspect the format of one of the fastqs. Notice that each read takes up 4 lines:\n",
    "1. the read name\n",
    "2. the read's nucleotide sequence\n",
    "3. a '+' to indicate the record contains another line\n",
    "4. a quality score for each base (a number encoded as a letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@NS500418:691:HTFJ7AFXX:1:11101:11481:1060 1:N:0:AAGAGGCA+GCGATCTA\r\n",
      "CTAAGAAGTGGATAACCAGCAAATGCTAGCACCACTATTTAGTAGGTTAAGGTCTCGTTCGTTATCGCAATTAAGC\r\n",
      "+\r\n",
      "AAAAAEEEEEEEEAEEEEEEEEEEEEE/EE/EEEEEEEEEEEEEEEEEEEEEEEEEEEEAEAEEEEEEEEEEA/EA\r\n",
      "@NS500418:691:HTFJ7AFXX:1:11101:12189:1060 1:N:0:AAGAGGCA+GCGATCTA\r\n",
      "CCTTCACCCAGGTAGGATAAGGATCAGGCGGAGCGACAGTATTAACAACAACTCGAGAAAAAACGATACATATACT\r\n",
      "+\r\n",
      "AAAAAEAEEAAE/EEEEEEEEEEEAEEAEAEEEAEAEA/EEEAEAEEEEA/EE<EAE/EEEA/AE//EAEEEEEAE\r\n",
      "\r\n",
      "gzip: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "zcat $(ls $FASTQ_DIR* | head -n 1) | head -n 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Adapter trimming\n",
    "\n",
    "- In many kinds of DNA and RNA sequencing experiments, sometimes the sequences will read through the targeted sequence insert and into sequencing adapter or PCR primer sequences on the end of the fragment. When the insert size is shorter than the read length (like in some of our ATAC-seq reads), the adapter sequence is read by the sequencer.\n",
    "\n",
    "- We need to remove such adapter sequences because they won't align to the genome.\n",
    "\n",
    "- In ATAC-seq (the data we're analyzing), the fragment length follows a periodic distribution. Some reads have very short inserts (only a few basepairs), while other reads have inserts that are much longer (100's of basepairs — much longer than the 77bp reads we're using to read them.\n",
    "\n",
    "- We know ahead of time that the first part of the adapter sequence is `CTGTCTCTTATA`, since our reads are sequenced using a Nextera sample prep kit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19440\r\n",
      "\r\n",
      "gzip: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "# Let's sanity check our adapter sequence by seeing\n",
    "# how many times it occurs in the first 100000 reads.\n",
    "\n",
    "ADAPTER=\"CTGTCTCTTATA\"\n",
    "\n",
    "NUM_LINES=400000  # 4 * num_reads, since each fastq entry is 4 lines\n",
    "\n",
    "zcat $(ls $FASTQ_DIR*R1* | head -n 1) | head -n $NUM_LINES | grep $ADAPTER | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r\n",
      "\r\n",
      "gzip: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "# Let's also check how often a permutation (rearrangement)\n",
    "# of the adapter sequence occurs:\n",
    "\n",
    "NOT_ADAPTER=\"CGTTCTTCTATA\"  # A permutation of the adapter sequence\n",
    "\n",
    "zcat $(ls $FASTQ_DIR*R1* | head -n 1) | head -n $NUM_LINES | grep $NOT_ADAPTER | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the correct adapter sequence occurs *many* times more in the reads than a permutation of the adapter sequene — this is an important validation that we have the right sequence.\n",
    "\n",
    "Now, we'll trim the paired-end reads using a tool called `cutadapt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "#create a directory to store the trimmed data \n",
    "export TRIMMED_DIR=\"$ANALYSIS_DIR/trimmed/\"\n",
    "[[ ! -d $TRIMMED_DIR ]] && mkdir -p \"$TRIMMED_DIR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for R1_fastq in ${FASTQ_DIR}*_R1*fastq.gz; do\n",
    "    \n",
    "    # Get the read 2 fastq file from the filename of read 1\n",
    "    R2_fastq=$(echo $R1_fastq | sed -e 's/R1/R2/')\n",
    "    \n",
    "    # Generate names for the trimmed fastq files\n",
    "\n",
    "    trimmed_R1_fastq=$TRIMMED_DIR$(echo $(basename $R1_fastq)| sed -e 's/.fastq.gz/.trimmed.fastq.gz/')\n",
    "    trimmed_R2_fastq=$TRIMMED_DIR$(echo $(basename $R2_fastq)| sed -e 's/.fastq.gz/.trimmed.fastq.gz/')   \n",
    "    echo cutadapt -m 5 -e 0.20 -a CTGTCTCTTATA -A CTGTCTCTTATA \\\n",
    "        -o ${trimmed_R1_fastq} \\\n",
    "        -p ${trimmed_R2_fastq} \\\n",
    "        $R1_fastq \\\n",
    "        $R2_fastq\n",
    "    cutadapt -m 5 -e 0.20 -a CTGTCTCTTATA -A CTGTCTCTTATA \\\n",
    "        -o ${trimmed_R1_fastq} \\\n",
    "        -p ${trimmed_R2_fastq} \\\n",
    "        $R1_fastq \\\n",
    "        $R2_fastq\n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Alignment\n",
    "\n",
    "Now, we're ready to align our trimmed reads to the Yeast SacCer3 reference genome.\n",
    "\n",
    "We'll use [Bowtie2](http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml), which is a [Burrows-Wheeler](https://en.wikipedia.org/wiki/Burrows%E2%80%93Wheeler_transform) based spliced aligner.\n",
    "\n",
    "Bowtie2 outputs a SAM (Sequence Alignment Map) file, which is a standard text encoding. To save space, we'll use `samtools view -b` to encode the output as a binarized SAM file — a BAM file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srv/scratch/training_camp/saccer3/bowtie2_index/saccer3\r\n"
     ]
    }
   ],
   "source": [
    "#set the bowtie index\n",
    "export bowtie_index=$YEAST_INDEX\n",
    "echo $bowtie_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "#create a directory to store the aligned data \n",
    "export ALIGNMENT_DIR=\"$ANALYSIS_DIR/aligned/\"\n",
    "[[ ! -d $ALIGNMENT_DIR ]] && mkdir -p \"$ALIGNMENT_DIR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[samopen] SAM header is present: 17 sequences.\r\n",
      "1508102 reads; of these:\r\n",
      "  1508102 (100.00%) were paired; of these:\r\n",
      "    73214 (4.85%) aligned concordantly 0 times\r\n",
      "    630015 (41.78%) aligned concordantly exactly 1 time\r\n",
      "    804873 (53.37%) aligned concordantly >1 times\r\n",
      "    ----\r\n",
      "    73214 pairs aligned concordantly 0 times; of these:\r\n",
      "      8601 (11.75%) aligned discordantly 1 time\r\n",
      "    ----\r\n",
      "    64613 pairs aligned 0 times concordantly or discordantly; of these:\r\n",
      "      129226 mates make up the pairs; of these:\r\n",
      "        90618 (70.12%) aligned 0 times\r\n",
      "        8547 (6.61%) aligned exactly 1 time\r\n",
      "        30061 (23.26%) aligned >1 times\r\n",
      "97.00% overall alignment rate\r\n",
      "[samopen] SAM header is present: 17 sequences.\r\n",
      "2998028 reads; of these:\r\n",
      "  2998028 (100.00%) were paired; of these:\r\n",
      "    152546 (5.09%) aligned concordantly 0 times\r\n",
      "    1522929 (50.80%) aligned concordantly exactly 1 time\r\n",
      "    1322553 (44.11%) aligned concordantly >1 times\r\n",
      "    ----\r\n",
      "    152546 pairs aligned concordantly 0 times; of these:\r\n",
      "      19173 (12.57%) aligned discordantly 1 time\r\n",
      "    ----\r\n",
      "    133373 pairs aligned 0 times concordantly or discordantly; of these:\r\n",
      "      266746 mates make up the pairs; of these:\r\n",
      "        204475 (76.66%) aligned 0 times\r\n",
      "        14991 (5.62%) aligned exactly 1 time\r\n",
      "        47280 (17.72%) aligned >1 times\r\n",
      "96.59% overall alignment rate\r\n",
      "[samopen] SAM header is present: 17 sequences.\r\n",
      "601214 reads; of these:\r\n",
      "  601214 (100.00%) were paired; of these:\r\n",
      "    65890 (10.96%) aligned concordantly 0 times\r\n",
      "    334444 (55.63%) aligned concordantly exactly 1 time\r\n",
      "    200880 (33.41%) aligned concordantly >1 times\r\n",
      "    ----\r\n",
      "    65890 pairs aligned concordantly 0 times; of these:\r\n",
      "      4962 (7.53%) aligned discordantly 1 time\r\n",
      "    ----\r\n",
      "    60928 pairs aligned 0 times concordantly or discordantly; of these:\r\n",
      "      121856 mates make up the pairs; of these:\r\n",
      "        110272 (90.49%) aligned 0 times\r\n",
      "        2826 (2.32%) aligned exactly 1 time\r\n",
      "        8758 (7.19%) aligned >1 times\r\n",
      "90.83% overall alignment rate\r\n",
      "[samopen] SAM header is present: 17 sequences.\r\n",
      "3274076 reads; of these:\r\n",
      "  3274076 (100.00%) were paired; of these:\r\n",
      "    249700 (7.63%) aligned concordantly 0 times\r\n",
      "    1902020 (58.09%) aligned concordantly exactly 1 time\r\n",
      "    1122356 (34.28%) aligned concordantly >1 times\r\n",
      "    ----\r\n",
      "    249700 pairs aligned concordantly 0 times; of these:\r\n",
      "      26069 (10.44%) aligned discordantly 1 time\r\n",
      "    ----\r\n",
      "    223631 pairs aligned 0 times concordantly or discordantly; of these:\r\n",
      "      447262 mates make up the pairs; of these:\r\n",
      "        377700 (84.45%) aligned 0 times\r\n",
      "        18447 (4.12%) aligned exactly 1 time\r\n",
      "        51115 (11.43%) aligned >1 times\r\n",
      "94.23% overall alignment rate\r\n",
      "[samopen] SAM header is present: 17 sequences.\r\n",
      "3876571 reads; of these:\r\n",
      "  3876571 (100.00%) were paired; of these:\r\n",
      "    270856 (6.99%) aligned concordantly 0 times\r\n",
      "    2324735 (59.97%) aligned concordantly exactly 1 time\r\n",
      "    1280980 (33.04%) aligned concordantly >1 times\r\n",
      "    ----\r\n",
      "    270856 pairs aligned concordantly 0 times; of these:\r\n",
      "      32846 (12.13%) aligned discordantly 1 time\r\n",
      "    ----\r\n",
      "    238010 pairs aligned 0 times concordantly or discordantly; of these:\r\n",
      "      476020 mates make up the pairs; of these:\r\n",
      "        400374 (84.11%) aligned 0 times\r\n",
      "        24033 (5.05%) aligned exactly 1 time\r\n",
      "        51613 (10.84%) aligned >1 times\r\n",
      "94.84% overall alignment rate\r\n",
      "[samopen] SAM header is present: 17 sequences.\r\n",
      "\r\n",
      "gzip: /srv/scratch/training_camp/tc2017/user1/analysis//trimmed/WT-SCE-0_6MNaCl-Rep2_R1_001.trimmed.fastq.gz: invalid compressed data--crc error\r\n",
      "\r\n",
      "gzip: /srv/scratch/training_camp/tc2017/user1/analysis//trimmed/WT-SCE-0_6MNaCl-Rep2_R1_001.trimmed.fastq.gz: invalid compressed data--length error\r\n",
      "Error, fewer reads in file specified with -2 than in file specified with -1\r\n",
      "terminate called after throwing an instance of 'int'\r\n",
      "bowtie2-align died with signal 6 (ABRT) (core dumped)\r\n",
      "[samopen] SAM header is present: 17 sequences.\r\n",
      "\r\n",
      "gzip: /srv/scratch/training_camp/tc2017/user1/analysis//trimmed/WT-SCE-Rep1_R2_001.trimmed.fastq.gz: unexpected end of file\r\n",
      "\r\n",
      "gzip: /srv/scratch/training_camp/tc2017/user1/analysis//trimmed/WT-SCE-Rep1_R1_001.trimmed.fastq.gz: unexpected end of file\r\n",
      "Error, fewer reads in file specified with -2 than in file specified with -1\r\n",
      "terminate called after throwing an instance of 'int'\r\n",
      "bowtie2-align died with signal 6 (ABRT) (core dumped)\r\n",
      "[samopen] SAM header is present: 17 sequences.\r\n",
      "4059565 reads; of these:\r\n",
      "  4059565 (100.00%) were paired; of these:\r\n",
      "    282359 (6.96%) aligned concordantly 0 times\r\n",
      "    2500637 (61.60%) aligned concordantly exactly 1 time\r\n",
      "    1276569 (31.45%) aligned concordantly >1 times\r\n",
      "    ----\r\n",
      "    282359 pairs aligned concordantly 0 times; of these:\r\n",
      "      35448 (12.55%) aligned discordantly 1 time\r\n",
      "    ----\r\n",
      "    246911 pairs aligned 0 times concordantly or discordantly; of these:\r\n",
      "      493822 mates make up the pairs; of these:\r\n",
      "        414767 (83.99%) aligned 0 times\r\n",
      "        26614 (5.39%) aligned exactly 1 time\r\n",
      "        52441 (10.62%) aligned >1 times\r\n",
      "94.89% overall alignment rate\r\n",
      "[samopen] SAM header is present: 17 sequences.\r\n",
      "2022 reads; of these:\r\n",
      "  2022 (100.00%) were paired; of these:\r\n",
      "    1892 (93.57%) aligned concordantly 0 times\r\n",
      "    78 (3.86%) aligned concordantly exactly 1 time\r\n",
      "    52 (2.57%) aligned concordantly >1 times\r\n",
      "    ----\r\n",
      "    1892 pairs aligned concordantly 0 times; of these:\r\n",
      "      1 (0.05%) aligned discordantly 1 time\r\n",
      "    ----\r\n",
      "    1891 pairs aligned 0 times concordantly or discordantly; of these:\r\n",
      "      3782 mates make up the pairs; of these:\r\n",
      "        3776 (99.84%) aligned 0 times\r\n",
      "        2 (0.05%) aligned exactly 1 time\r\n",
      "        4 (0.11%) aligned >1 times\r\n",
      "6.63% overall alignment rate\r\n",
      "[samopen] SAM header is present: 17 sequences.\r\n",
      "974 reads; of these:\r\n",
      "  974 (100.00%) were paired; of these:\r\n",
      "    524 (53.80%) aligned concordantly 0 times\r\n",
      "    244 (25.05%) aligned concordantly exactly 1 time\r\n",
      "    206 (21.15%) aligned concordantly >1 times\r\n",
      "    ----\r\n",
      "    524 pairs aligned concordantly 0 times; of these:\r\n",
      "      7 (1.34%) aligned discordantly 1 time\r\n",
      "    ----\r\n",
      "    517 pairs aligned 0 times concordantly or discordantly; of these:\r\n",
      "      1034 mates make up the pairs; of these:\r\n",
      "        1020 (98.65%) aligned 0 times\r\n",
      "        2 (0.19%) aligned exactly 1 time\r\n",
      "        12 (1.16%) aligned >1 times\r\n",
      "47.64% overall alignment rate\r\n",
      "[samopen] SAM header is present: 17 sequences.\r\n",
      "2604271 reads; of these:\r\n",
      "  2604271 (100.00%) were paired; of these:\r\n",
      "    245197 (9.42%) aligned concordantly 0 times\r\n",
      "    1521101 (58.41%) aligned concordantly exactly 1 time\r\n",
      "    837973 (32.18%) aligned concordantly >1 times\r\n",
      "    ----\r\n",
      "    245197 pairs aligned concordantly 0 times; of these:\r\n",
      "      20216 (8.24%) aligned discordantly 1 time\r\n",
      "    ----\r\n",
      "    224981 pairs aligned 0 times concordantly or discordantly; of these:\r\n",
      "      449962 mates make up the pairs; of these:\r\n",
      "        399028 (88.68%) aligned 0 times\r\n",
      "        14651 (3.26%) aligned exactly 1 time\r\n",
      "        36283 (8.06%) aligned >1 times\r\n",
      "92.34% overall alignment rate\r\n",
      "[samopen] SAM header is present: 17 sequences.\r\n",
      "\r\n",
      "gzip: /srv/scratch/training_camp/tc2017/user1/analysis//trimmed/cln3-SCD-Rep2_R1_001.trimmed.fastq.gz: invalid compressed data--crc error\r\n",
      "\r\n",
      "gzip: /srv/scratch/training_camp/tc2017/user1/analysis//trimmed/cln3-SCD-Rep2_R1_001.trimmed.fastq.gz: invalid compressed data--length error\r\n",
      "Error, fewer reads in file specified with -2 than in file specified with -1\r\n",
      "terminate called after throwing an instance of 'int'\r\n",
      "bowtie2-align died with signal 6 (ABRT) (core dumped)\r\n",
      "[samopen] SAM header is present: 17 sequences.\r\n",
      "bowtie2-align died with signal 2 (INT) \r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "for trimmed_fq1 in ${TRIMMED_DIR}*_R1*fastq.gz; do\n",
    "\n",
    "    trimmed_fq2=$(echo $trimmed_fq1 | sed -e 's/_R1/_R2/')\n",
    "    \n",
    "    bam=$(echo \"${ALIGNMENT_DIR}${trimmed_fq1##*/}\" | sed -e 's/.fastq.gz/.bam/')\n",
    "    bowtie2 -X2000 --mm --threads 10 -x $bowtie_index -1 $trimmed_fq1 -2 $trimmed_fq2 | samtools view -bS - > $bam        \n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[samopen] SAM header is present: 17 sequences.\n",
    "4376854 reads; of these:\n",
    "  4376854 (100.00%) were paired; of these:\n",
    "    169693 (3.88%) aligned concordantly 0 times\n",
    "    2813168 (64.27%) aligned concordantly exactly 1 time\n",
    "    1393993 (31.85%) aligned concordantly >1 times\n",
    "    ----\n",
    "    169693 pairs aligned concordantly 0 times; of these:\n",
    "      37019 (21.82%) aligned discordantly 1 time\n",
    "    ----\n",
    "    132674 pairs aligned 0 times concordantly or discordantly; of these:\n",
    "      265348 mates make up the pairs; of these:\n",
    "        181304 (68.33%) aligned 0 times\n",
    "        29265 (11.03%) aligned exactly 1 time\n",
    "        54779 (20.64%) aligned >1 times\n",
    "97.93% overall alignment rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Finding duplicate reads and alignment filtering\n",
    "\n",
    "During sequencing, we perform PCR, which can lead to duplicate reads. In many kinds of DNA sequencing, we want to remove duplicates so that we don't double-count signal originating from the same molecule.\n",
    "\n",
    "To do so, we use an algorithm called `sambamba` that looks for reads that mapped to exactly the same places in the genome. We also need to sort the aligned files before we can mark duplicates, since we need reads aligned to the same position to be next to each other in the file.\n",
    "\n",
    "Bowtie2 also sets certian labels (or \"flags\") in the resulting alignment file to indicate information like the score of the alignment, the orientation of both mates of the fragment, and other details.\n",
    "\n",
    "We can use these flags as a way to discard low-quality reads. [This website](https://broadinstitute.github.io/picard/explain-flags.html) provides a convenient way to interpret the meaning of these bitwise flags; for conveninece they can be encoded as numbers.\n",
    "\n",
    "Here, we want to filter reads that fall into any of the following categories:\n",
    "- the read wasn't mapped to the genome\n",
    "- the read's mate wasn't mapped to the genome\n",
    "- the alignment reported is not the primary alignment (it is a \"runner-up\" alignment)\n",
    "- the read was marked as \"low-quality\" by the sequencer software\n",
    "- the read has a mapping quality less than 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding positions of the duplicate reads in the file...\r\n",
      "  sorted 1455280 end pairs\r\n",
      "     and 15026 single ends (among them 0 unmatched pairs)\r\n",
      "  collecting indices of duplicate reads...   done in 229 ms\r\n",
      "  found 956072 duplicates\r\n",
      "collected list of positions in 0 min 8 sec\r\n",
      "marking duplicates...\r\n",
      "total time elapsed: 0 min 15 sec\r\n",
      "[bam_index_build2] fail to create the index file.\r\n",
      "finding positions of the duplicate reads in the file...\r\n",
      "  sorted 2883323 end pairs\r\n",
      "     and 24935 single ends (among them 0 unmatched pairs)\r\n",
      "  collecting indices of duplicate reads...   done in 422 ms\r\n",
      "  found 1728829 duplicates\r\n",
      "collected list of positions in 0 min 17 sec\r\n",
      "marking duplicates...\r\n",
      "total time elapsed: 0 min 30 sec\r\n",
      "[bam_index_build2] fail to create the index file.\r\n",
      "finding positions of the duplicate reads in the file...\r\n",
      "  sorted 543951 end pairs\r\n",
      "     and 4254 single ends (among them 0 unmatched pairs)\r\n",
      "  collecting indices of duplicate reads...   done in 76 ms\r\n",
      "  found 153659 duplicates\r\n",
      "collected list of positions in 0 min 3 sec\r\n",
      "marking duplicates...\r\n",
      "total time elapsed: 0 min 5 sec\r\n",
      "[bam_index_build2] fail to create the index file.\r\n",
      "finding positions of the duplicate reads in the file...\r\n",
      "  sorted 3071531 end pairs\r\n",
      "     and 27390 single ends (among them 0 unmatched pairs)\r\n",
      "  collecting indices of duplicate reads...   done in 443 ms\r\n",
      "  found 1528076 duplicates\r\n",
      "collected list of positions in 0 min 16 sec\r\n",
      "marking duplicates...\r\n",
      "total time elapsed: 0 min 30 sec\r\n",
      "[bam_index_build2] fail to create the index file.\r\n",
      "finding positions of the duplicate reads in the file...\r\n",
      "  sorted 3658944 end pairs\r\n",
      "     and 34880 single ends (among them 0 unmatched pairs)\r\n",
      "  collecting indices of duplicate reads...   done in 517 ms\r\n",
      "  found 1738693 duplicates\r\n",
      "collected list of positions in 0 min 20 sec\r\n",
      "marking duplicates...\r\n",
      "total time elapsed: 0 min 37 sec\r\n",
      "[bam_index_build2] fail to create the index file.\r\n",
      "finding positions of the duplicate reads in the file...\r\n",
      "  sorted 234394 end pairs\r\n",
      "     and 2557 single ends (among them 0 unmatched pairs)\r\n",
      "  collecting indices of duplicate reads...   done in 32 ms\r\n",
      "  found 32351 duplicates\r\n",
      "collected list of positions in 0 min 1 sec\r\n",
      "marking duplicates...\r\n",
      "total time elapsed: 0 min 2 sec\r\n",
      "[bam_index_build2] fail to create the index file.\r\n",
      "finding positions of the duplicate reads in the file...\r\n",
      "  sorted 32375 end pairs\r\n",
      "     and 184 single ends (among them 0 unmatched pairs)\r\n",
      "  collecting indices of duplicate reads...   done in 4 ms\r\n",
      "  found 3033 duplicates\r\n",
      "collected list of positions in 0 min 0 sec\r\n",
      "marking duplicates...\r\n",
      "total time elapsed: 0 min 0 sec\r\n",
      "[bam_index_build2] fail to create the index file.\r\n",
      "finding positions of the duplicate reads in the file...\r\n",
      "  sorted 3833173 end pairs\r\n",
      "     and 38017 single ends (among them 0 unmatched pairs)\r\n",
      "  collecting indices of duplicate reads...   done in 548 ms\r\n",
      "  found 1720072 duplicates\r\n",
      "collected list of positions in 0 min 22 sec\r\n",
      "marking duplicates...\r\n",
      "total time elapsed: 0 min 40 sec\r\n",
      "[bam_index_build2] fail to create the index file.\r\n",
      "finding positions of the duplicate reads in the file...\r\n",
      "  sorted 132 end pairs\r\n",
      "     and 4 single ends (among them 0 unmatched pairs)\r\n",
      "  collecting indices of duplicate reads...   done in 0 ms\r\n",
      "  found 6 duplicates\r\n",
      "collected list of positions in 0 min 0 sec\r\n",
      "marking duplicates...\r\n",
      "total time elapsed: 0 min 0 sec\r\n",
      "[bam_index_build2] fail to create the index file.\r\n",
      "finding positions of the duplicate reads in the file...\r\n",
      "  sorted 463 end pairs\r\n",
      "     and 2 single ends (among them 0 unmatched pairs)\r\n",
      "  collecting indices of duplicate reads...   done in 0 ms\r\n",
      "  found 4 duplicates\r\n",
      "collected list of positions in 0 min 0 sec\r\n",
      "marking duplicates...\r\n",
      "total time elapsed: 0 min 0 sec\r\n",
      "[bam_index_build2] fail to create the index file.\r\n",
      "finding positions of the duplicate reads in the file...\r\n",
      "  sorted 2393948 end pairs\r\n",
      "     and 21618 single ends (among them 0 unmatched pairs)\r\n",
      "  collecting indices of duplicate reads...   done in 345 ms\r\n",
      "  found 1023704 duplicates\r\n",
      "collected list of positions in 0 min 12 sec\r\n",
      "marking duplicates...\r\n",
      "total time elapsed: 0 min 23 sec\r\n",
      "[bam_index_build2] fail to create the index file.\r\n",
      "finding positions of the duplicate reads in the file...\r\n",
      "  sorted 1461857 end pairs\r\n",
      "     and 13222 single ends (among them 0 unmatched pairs)\r\n",
      "  collecting indices of duplicate reads...   done in 208 ms\r\n",
      "  found 572205 duplicates\r\n",
      "collected list of positions in 0 min 6 sec\r\n",
      "marking duplicates...\r\n",
      "total time elapsed: 0 min 13 sec\r\n",
      "[bam_index_build2] fail to create the index file.\r\n",
      "sambamba-sort: Error reading BGZF block starting from offset 420243315: stream error: not enough data in stream\r\n",
      "finding positions of the duplicate reads in the file...\r\n",
      "  sorted 3615639 end pairs\r\n",
      "     and 32452 single ends (among them 0 unmatched pairs)\r\n",
      "  collecting indices of duplicate reads...   done in 550 ms\r\n",
      "  found 1728134 duplicates\r\n",
      "collected list of positions in 0 min 23 sec\r\n",
      "marking duplicates...\r\n",
      "total time elapsed: 0 min 40 sec\r\n",
      "[bam_index_build2] fail to create the index file.\r\n",
      "finding positions of the duplicate reads in the file...\r\n",
      "  sorted 2869215 end pairs\r\n",
      "     and 27779 single ends (among them 0 unmatched pairs)\r\n",
      "  collecting indices of duplicate reads...   done in 430 ms\r\n",
      "  found 1747654 duplicates\r\n",
      "collected list of positions in 0 min 16 sec\r\n",
      "marking duplicates...\r\n",
      "total time elapsed: 0 min 29 sec\r\n",
      "[bam_index_build2] fail to create the index file.\r\n",
      "finding positions of the duplicate reads in the file...\r\n",
      "  sorted 3614765 end pairs\r\n",
      "     and 27110 single ends (among them 0 unmatched pairs)\r\n",
      "  collecting indices of duplicate reads...   done in 521 ms\r\n",
      "  found 1881724 duplicates\r\n",
      "collected list of positions in 0 min 19 sec\r\n",
      "marking duplicates...\r\n",
      "total time elapsed: 0 min 35 sec\r\n",
      "[bam_index_build2] fail to create the index file.\r\n",
      "finding positions of the duplicate reads in the file...\r\n",
      "  sorted 1669000 end pairs\r\n",
      "     and 12477 single ends (among them 0 unmatched pairs)\r\n",
      "  collecting indices of duplicate reads...   done in 242 ms\r\n",
      "  found 720993 duplicates\r\n",
      "collected list of positions in 0 min 8 sec\r\n",
      "marking duplicates...\r\n",
      "total time elapsed: 0 min 16 sec\r\n",
      "[bam_index_build2] fail to create the index file.\r\n",
      "finding positions of the duplicate reads in the file...\r\n",
      "  sorted 3553952 end pairs\r\n",
      "     and 35068 single ends (among them 0 unmatched pairs)\r\n",
      "  collecting indices of duplicate reads...   done in 496 ms\r\n",
      "  found 1381273 duplicates\r\n",
      "collected list of positions in 0 min 17 sec\r\n",
      "marking duplicates...\r\n",
      "total time elapsed: 0 min 33 sec\r\n",
      "[bam_index_build2] fail to create the index file.\r\n",
      "finding positions of the duplicate reads in the file...\r\n",
      "  sorted 3977362 end pairs\r\n",
      "     and 35702 single ends (among them 0 unmatched pairs)\r\n",
      "  collecting indices of duplicate reads...   done in 548 ms\r\n",
      "  found 1193148 duplicates\r\n",
      "collected list of positions in 0 min 22 sec\r\n",
      "marking duplicates...\r\n",
      "total time elapsed: 0 min 40 sec\r\n",
      "[bam_index_build2] fail to create the index file.\r\n",
      "finding positions of the duplicate reads in the file...\r\n",
      "  sorted 4456061 end pairs\r\n",
      "     and 40617 single ends (among them 0 unmatched pairs)\r\n",
      "  collecting indices of duplicate reads...   done in 634 ms\r\n",
      "  found 2079803 duplicates\r\n",
      "collected list of positions in 0 min 24 sec\r\n",
      "marking duplicates...\r\n",
      "total time elapsed: 0 min 44 sec\r\n",
      "[bam_index_build2] fail to create the index file.\r\n",
      "finding positions of the duplicate reads in the file...\r\n",
      "  sorted 4265348 end pairs\r\n",
      "     and 41708 single ends (among them 0 unmatched pairs)\r\n",
      "  collecting indices of duplicate reads...   done in 607 ms\r\n",
      "  found 1824874 duplicates\r\n",
      "collected list of positions in 0 min 22 sec\r\n",
      "marking duplicates...\r\n",
      "total time elapsed: 0 min 41 sec\r\n",
      "[bam_index_build2] fail to create the index file.\r\n"
     ]
    }
   ],
   "source": [
    "for bam_file in ${ALIGNMENT_DIR}*.trimmed.bam; do\n",
    "\n",
    "    bam_file_sorted=$(echo $bam_file | sed -e 's/.bam/.sorted.bam/')\n",
    "    bam_file_dup=$(echo $bam_file | sed -e 's/.bam/.sorted.dup.bam/')\n",
    "    nodup_bam_file=$(echo $bam_file | sed -e 's/.bam/.nodup.bam/')\n",
    "    \n",
    "    # Sort and remove duplicates\n",
    "    sambamba sort -m 4G -t 40 -u $bam_file \n",
    "    sambamba markdup -l 0 -t 40 $bam_file_sorted $bam_file_dup\n",
    "    samtools view -F 1804 -f 2 -q 30 -b $bam_file_dup  > $nodup_bam_file\n",
    "    samtools index $nodup_bam_file\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Peak calling\n",
    "\n",
    "Now that we've aligned our reads to the genome and filtered the alignments, we want to identify areas of locally enriched signals, or \"peaks\".\n",
    "\n",
    "For ATAC-seq, peaks correspond to accessible regions. They can include promoters, enhancers, and other regulatory regions.\n",
    "\n",
    "We'll call peaks using [MACS2](http://liulab.dfci.harvard.edu/MACS/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a directory to store the tagAlign data \n",
    "TAGALIGN_DIR=\"${ANALYSIS_DIR}tagAlign/\"\n",
    "[[ ! -d $TAGALIGN_DIR ]] && mkdir -p \"$TAGALIGN_DIR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a directory to store the MACS peaks \n",
    "PEAKS_DIR=\"${ANALYSIS_DIR}peaks/\"\n",
    "[[ ! -d $PEAKS_DIR ]] && mkdir -p \"$PEAKS_DIR\"\n",
    "echo $PEAKS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SacCer3GenSz=12157105  # The sum of the sizes of the chromosomes in the SacCer3 genome\n",
    "\n",
    "Macs2PvalThresh=\"0.05\"  # The p-value threshold for calling peaks \n",
    "\n",
    "Macs2SmoothWindow=150  # The window size to smooth alignment signal over\n",
    "Macs2ShiftSize=$(python -c \"print(int(${Macs2SmoothWindow}/2))\")\n",
    "\n",
    "for nodup_bam_file in ${ALIGNMENT_DIR}*.nodup.bam; do\n",
    "    \n",
    "    # First, we need to convert each bam to a .tagAlign,\n",
    "    # which just contains the start/end positions of each read:\n",
    "    \n",
    "    tagalign_file=$TAGALIGN_DIR$(echo $(basename $nodup_bam_file) | sed -e 's/.bam/.tagAlign.gz/')\n",
    "    bedtools bamtobed -i $nodup_bam_file | awk 'BEGIN{OFS=\"\\t\"}{$4=\"N\";$5=\"1000\";print $0}' | gzip -c > $tagalign_file\n",
    "    \n",
    "    # Now, we can run MACS:\n",
    "    output_prefix=$PEAKS_DIR$(echo $(basename $tagalign_file)| sed -e 's/.tagAlign.gz//')\n",
    "     macs2 callpeak \\\n",
    "        -t $tagalign_file -f BED -n $output_prefix -g \"$SacCer3GenSz\" -p $Macs2PvalThresh \\\n",
    "        --nomodel --shift -$Macs2ShiftSize --extsize $Macs2SmoothWindow -B --SPMR --keep-dup all --call-summits\n",
    "\n",
    "    #We also generate a fold change file comparing the sample to the control(DMSO)\n",
    "    macs2 bdgcmp -t $output_prefix\\_treat_pileup.bdg -c $output_prefix\\_control_lambda.bdg -o $output_prefix\\_FE.bdg -m FE\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we merge the peaks across all conditions to create a master list of peaks for analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $PEAKS_DIR\n",
    "#concatenate all .narrowPeak files together \n",
    "cat *narrowPeak > all.peaks.bed \n",
    "\n",
    "#sort the concatenated file \n",
    "bedtools sort -i all.peaks.bed > all.peaks.sorted.bed \n",
    "\n",
    "#merge the sorted, concatenated fileto join overlapping peaks \n",
    "\n",
    "cat all.peaks.sorted.bed | awk -F '\\t' 'BEGIN {{ OFS=\"\\t\" }} {{ $2=$2+$10-100; $3=$2+$10+100; if ($2<0) {{$2 = 0}} print $0 }}' | bedtools sort | bedtools merge > all_merged.peaks.bed\n",
    "gzip -f all_merged.peaks.bed \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
