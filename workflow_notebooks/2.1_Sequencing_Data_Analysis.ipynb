{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequencing data analysis\n",
    "\n",
    "### IMPORTANT: Please make sure that your are using the bash kernel to run this notebook.\n",
    "#### (Do this at the beginning of every session) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook covers analysis of DNA sequencing data from raw files to processed signals.\n",
    "\n",
    "Although this analysis is for ATAC-seq data, many of the steps (especially the first section) are the same for other types of DNA sequencing experiments.\n",
    "\n",
    "We'll be doing the analysis in Bash, which is the standard language for UNIX command-line scripting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps in the analysis pipeline that are covered in this notebook are indicated below:\n",
    "![Sequencing Data Analysis 1](images/part1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting up the data\n",
    "\n",
    "We start with raw `.fastq.gz` files, which are provided by the sequencing instrument. For each DNA molecule (read) that was sequenced, they provide the nucleotide sequence, and information about the quality of the signal of that nucleotide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "### Set up variables storing the location of our data\n",
    "### The proper way to load your variables is with the ~/.bashrc command, but this is very slow in iPython \n",
    "export SUNETID=\"$(whoami)\"\n",
    "export WORK_DIR=\"/srv/scratch/training_camp/work/${SUNETID}\"\n",
    "export DATA_DIR=\"${WORK_DIR}/data\"\n",
    "[[ ! -d ${WORK_DIR}/data ]] && mkdir \"${WORK_DIR}/data\"\n",
    "export SRC_DIR=\"${WORK_DIR}/src\"\n",
    "[[ ! -d ${WORK_DIR}/src ]] && mkdir -p \"${WORK_DIR}/src\"\n",
    "export METADATA_DIR=\"/srv/scratch/training_camp/metadata\"\n",
    "export AGGREGATE_DATA_DIR=\"/srv/scratch/training_camp/data\"\n",
    "export AGGREGATE_ANALYSIS_DIR=\"/srv/scratch/training_camp/data\"\n",
    "export YEAST_DIR=\"/srv/scratch/training_camp/saccer3\"\n",
    "export TMP=\"${WORK_DIR}/tmp\"\n",
    "export TEMP=$TMP\n",
    "export TMPDIR=$TMP\n",
    "[[ ! -d ${TMP} ]] && mkdir -p \"${TMP}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check exactly which fastqs we have (we copied these from $AGGREGATE_DATA_DIR to your personal $DATA_DIR in the last tutorial):\n",
    "\n",
    "(recall that the `ls` command lists the contents of a directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls $DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As a sanity check, we can also look at the size and last edited time of some of the fastqs by addind `-lrth` to the `ls` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 383M\r\n",
      "-rwxrwxr-x 1 ubuntu ubuntu  32M Sep  8 02:05 WT-SCD-Rep1_R1_001.fastq.gz\r\n",
      "-rwxrwxr-x 1 ubuntu ubuntu  28M Sep  8 02:05 WT-SCD-Rep1_R2_001.fastq.gz\r\n",
      "-rwxrwxr-x 1 ubuntu ubuntu 168M Sep  8 02:06 WT-SCD-Rep2_R1_001.fastq.gz\r\n",
      "-rwxrwxr-x 1 ubuntu ubuntu 157M Sep  8 02:06 WT-SCD-Rep2_R2_001.fastq.gz\r\n"
     ]
    }
   ],
   "source": [
    "ls -lrth $DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also inspect the format of one of the fastqs. Notice that each read takes up 4 lines:\n",
    "1. the read name\n",
    "2. the read's nucleotide sequence\n",
    "3. a '+' to indicate the record contains another line\n",
    "4. a quality score for each base (a number encoded as a letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@NS500418:691:HTFJ7AFXX:1:11101:13955:1071 1:N:0:CTCTCTAC+GCGATCTA\r\n",
      "TCTCTATGATGGTAATAGGCAAACATCGGGCGTACCTTAAAAGTCTTAGACATCACATAAACTGTCTCTTATACAC\r\n",
      "+\r\n",
      "AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEAEEEAEEEEEEEEE\r\n",
      "@NS500418:691:HTFJ7AFXX:1:11101:20718:1090 1:N:0:CTCTCTAC+GCGATCTA\r\n",
      "CCCCCTCCCATTACAAACTAAAATCTTACTTTTATTTTCTTTTGCCCTCTCTGTCGCCTGTCTCTTATACACATCT\r\n",
      "+\r\n",
      "AAAAAEEEEEEEEEEEEAEEEEEEEEEEAEEEEEEEEE<EEEEEEEEAE/EEEEE/EEEEAE<6EEEAEEEAEEAE\r\n",
      "\r\n",
      "gzip: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "zcat $(ls $DATA_DIR/*gz | head -n 1) | head -n 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 2:ATAC-seq data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ENCODE consortium (https://www.encodeproject.org/) uses a standard ATAC-seq data processing pipeline, which can be downloaded here: https://github.com/ENCODE-DCC/atac-seq-pipeline\n",
    "\n",
    "This pipeline is pre-installed on this computer and can be executed by running the **atac.bds** script. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== atac pipeline settings\r\n",
      "\t-type <string>                   : Type of the pipeline. atac-seq or dnase-seq (default: atac-seq).\r\n",
      "\t-dnase_seq <bool>                : DNase-Seq (no tn5 shifting).\r\n",
      "\t-align <bool>                    : Align only (no MACS2 peak calling or IDR or ataqc analysis).\r\n",
      "\t-subsample_xcor <string>         : # reads to subsample for cross corr. analysis (default: 25M).\r\n",
      "\t-subsample <string>              : # reads to subsample exp. replicates. Subsampled tagalign will be used for steps downstream (default: 0; no subsampling).\r\n",
      "\t-true_rep <bool>                 : No pseudo-replicates.\r\n",
      "\t-no_ataqc <bool>                 : No ATAQC\r\n",
      "\t-no_xcor <bool>                  : No Cross-correlation analysis.\r\n",
      "\t-csem <bool>                     : Use CSEM for alignment.\r\n",
      "\t-smooth_win <string>             : Smoothing window size for MACS2 peak calling (default: 150).\r\n",
      "\t-idr_thresh <real>               : IDR threshold : -log_10(score) (default: 0.1).\r\n",
      "\t-ENCODE3 <bool>                  : Force to use parameter set (-smooth_win 73 -idr_thresh 0.05 -multimapping 4) for ENCODE3.\r\n",
      "\t-ENCODE <bool>                   : Force to use parameter set (-smooth_win 73 -idr_thresh 0.05 -multimapping 4) for ENCODE.\r\n",
      "\t-no_browser_tracks <bool>        : Disable generation of genome browser tracks (workaround for bzip2 shared library issue).\r\n",
      "\t-overlap_pval_thresh <real>      : p-val threshold for overlapped peaks (default: 0.01).\r\n",
      "\t-macs2_pval_thresh <real>        : MACS2 p-val threshold for calling peaks (default: 0.1).\r\n",
      "\t-macs2_pval_thresh_bw <real>     : MACS2 p-val threshold for generating BIGWIG signal tracks (default: 0.1).\r\n",
      "\t-enable_idr <bool>               : Enable IDR on called peaks.\r\n",
      "\t-auto_detect_adapter <bool>      : Automatically find and trim adapters.\r\n",
      "== configuration file settings\r\n",
      "\t-c <string>                      : Configuration file path.\r\n",
      "\t-env <string>                    : Environment file path.\r\n",
      "== parallelization settings\r\n",
      "\t-no_par <bool>                   : Serialize all tasks (individual tasks can still use multiple threads up to '-nth').\r\n",
      "\t-nth <int>                       : Maximum # threads for a pipeline. (default: 8).\r\n",
      "== cluster/system/resource settings\r\n",
      "\t-wt <string>                     : Walltime for all single-threaded tasks (example: 8:10:00, 3h, 3600, default: 5h50m, 5:50:00).\r\n",
      "\t-memory <string>                 : Maximum memory for all single-threaded tasks (equivalent to '-mem', example: 4.5G, 1024M, default: 7G).\r\n",
      "\t-use_system <string>             : Force to use a system (equivalent to 'bds -s [SYSTEM_NAME] ...', any system defined in bds.config can be used).\r\n",
      "\t-nice <int>                      : Set process priority for all tasks (default: 0; -20 (highest) ~ 19 (lowest) ).\r\n",
      "\t-retrial <int>                   : # of Retrial for failed tasks (default: 0).\r\n",
      "\t-q <string>                      : Submit tasks to a specified cluster queue.\r\n",
      "\t-q_for_slurm_account <bool>      : Use --account instead of -p (partition) for SLURM only.\r\n",
      "\t-unlimited_mem_wt <bool>         : Use unlimited max. memory and walltime.\r\n",
      "\t-java_tmp_dir <string>           : Java temporary directory. (change it when you get 'Disk quota exceeded' error in Java, default: ${TMPDIR}).\r\n",
      "== shell environment settings\r\n",
      "\t-mod <string>                    : Modules separated by ; (example: \"bowtie/2.2.4; bwa/0.7.7; picard-tools/1.92\").\r\n",
      "\t-shcmd <string>                  : Shell commands separated by ;. Shell var. must be written as ${VAR} not as $VAR (example: \"export PATH=${PATH}:/usr/test; VAR=test\").\r\n",
      "\t-addpath <string>                : Path separated by ; or : to be PREPENDED to \\$PATH (example: \"/bin/test:${HOME}/utils\").\r\n",
      "\t-conda_env <string>              : Anaconda Python (or Miniconda) environment name for all softwares including Python2.\r\n",
      "\t-conda_env_py3 <string>          : Anaconda Python (or Miniconda) environment name for Python3.\r\n",
      "\t-conda_bin_dir <string>          : Anaconda Python (or Miniconda) bin directory.\r\n",
      "\t-cluster_task_min_len <int>      : Minimum length for a cluster job in seconds (dealing with NFS delayed write, default: 60).\r\n",
      "\t-cluster_task_delay <int>        : Constant delay for every job in seconds (dealing with NFS delayed write, default: 0).\r\n",
      "== output/title settings\r\n",
      "\t-out_dir <string>                : Output directory (default: out).\r\n",
      "\t-title <string>                  : Prefix for HTML report and outputs without given prefix.\r\n",
      "== species settings\r\n",
      "\t-species <string>                : Species. need to specify '-species_file' too if you have not installed genome database with 'install_genome_data.sh'.\r\n",
      "\t-species_file <string>           : Species file path.\r\n",
      "\t-species_browser <string>        : Species name in WashU genome browser.\r\n",
      "\t-ref_fa <string>                 : Reference genome sequence fasta.\r\n",
      "\t-chrsz <string>                  : Chromosome sizes file path (use fetchChromSizes from UCSC tools).\r\n",
      "\t-blacklist <string>              : Blacklist bed.\r\n",
      "\t-seq_dir <string>                : Reference genome sequence directory path (where chr*.fa exist).\r\n",
      "== ENCODE accession settings\r\n",
      "\t-ENCODE_accession <string>       : ENCODE experiment accession ID (or dataset).\r\n",
      "\t-ENCODE_award_rfa <string>       : ENCODE award RFA (e.g. ENCODE3).\r\n",
      "\t-ENCODE_assay_category <string>  : ENCODE assay category.\r\n",
      "\t-ENCODE_assay_title <string>     : ENCODE assay title.\r\n",
      "\t-ENCODE_award <string>           : ENCODE award (e.g. /awards/U41HG007000/).\r\n",
      "\t-ENCODE_lab <string>             : Lab (e.g. /labs/anshul-kundaje/)\r\n",
      "\t-ENCODE_assembly <string>        : hg19, GRCh38, mm9, mm10.\r\n",
      "\t-ENCODE_alias_prefix <string>    : Alias prefix, Alias = alias_prefix: + filename + alias_suffix\r\n",
      "\t-ENCODE_alias_suffix <string>    : Alias suffix, Alias = alias_prefix: + filename + alias_suffix\r\n",
      "== report settings\r\n",
      "\t-url_base <string>               : URL base for output directory.\r\n",
      "\t-viz_genome_coord <string>       : WashU genome browser genome coordinate (e.g. chr7:27117661-27153380).\r\n",
      "== fastq input definition :\r\n",
      "        Single-ended : For replicate '-fastq[REP_ID]', For control '-ctl_fastq[REP_ID]'\r\n",
      "        Paired end : For replicate '-fastq[REP_ID]_[PAIR_ID]', For control '-ctl_fastq[REP_ID]_[PAIR_ID]'\r\n",
      "== bam input (raw or filtered) definition :\r\n",
      "        Raw bam : For replicate '-bam[REP_ID]', For control '-ctl_bam[REP_ID]'.\r\n",
      "        Filtered bam : For replicate '-filt_bam[REP_ID]', For control '-ctl_filt_bam[REP_ID]'.\r\n",
      "== tagalign input definition :\r\n",
      "        For replicate '-tag[REP_ID]', For control '-ctl_tag[REP_ID]'.\r\n",
      "== narrow peak input definition : \r\n",
      "        For true replicates, use '-peak1' and '-peak2',\r\n",
      "        For pooled replicates, use '-peak_pooled',\r\n",
      "        For two PR (self-pseudo-replicates), use '-peak[REP_ID]_pr1' and '-peak[REP_ID]_pr2'\r\n",
      "        For two PPR (pooled pseudo-replicates), use '-peak_ppr1' and '-peak_ppr2'\r\n",
      "== input endedness settings (SE or PE) :\r\n",
      "\t-se <bool>                       : Singled-ended data set. To specify it for each replicate, '-se[REP_ID]' for exp. reps, '-ctl_se[CTL_ID]' for control.\r\n",
      "\t-pe <bool>                       : Paired end data set. To specify it for each replicate, '-pe[REP_ID]' for exp. reps, '-ctl_pe[CTL_ID]' for controls.\r\n",
      "== adapter sequence definition :\r\n",
      "        Single-ended : For replicate '-adapter[REP_ID]'\r\n",
      "        Paired end : For replicate '-adapter[REP_ID]_[PAIR_ID]'\r\n",
      "== align multimapping settings\r\n",
      "\t-multimapping <int>              : # alignments reported for multimapping (default: 0).\r\n",
      "== align bowtie2 settings (requirements: -bwt2_idx)\r\n",
      "\t-bwt2_idx <string>               : Bowtie2 index (full path prefix of *.1.bt2 file).\r\n",
      "\t-scoremin_bwt2 <string>          : Replacement --score-min for bowtie2.\r\n",
      "\t-wt_bwt2 <string>                : Walltime for bowtie2 (default: 47h, 47:00:00).\r\n",
      "\t-mem_bwt2 <string>               : Max. memory for bowtie2 (default: 12G).\r\n",
      "\t-extra_param_bwt2 <string>       : Extra parameter for bowtie2.\r\n",
      "\t-no_idx_on_mem_bwt2 <bool>       : Disable loading index on memory by removing --mm flag for bowtie2.\r\n",
      "== adapter trimmer settings\r\n",
      "\t-adapter_err_rate <string>       : Maximum allowed adapter error rate (# errors divided by the length of the matching adapter region, default: 0.10).\r\n",
      "\t-min_trim_len <int>              : Minimum trim length for cutadapt -m, throwing away processed reads shorter than this (default: 5).\r\n",
      "\t-wt_trim <string>                : Walltime for adapter trimming (default: 23h, 23:00:00).\r\n",
      "\t-mem_trim <string>               : Max. memory for adapter trimming (default: 12G).\r\n",
      "== postalign bam settings\r\n",
      "\t-mapq_thresh <int>               : Threshold for low MAPQ reads removal (default: 30).\r\n",
      "\t-rm_chr_from_tag <string>        : Perl style reg-ex to exclude reads from tag-aligns. (example: 'other|ribo|mito|_', '_', default: blank)\r\n",
      "\t-no_dup_removal <bool>           : No dupe removal when filtering raw bam.\r\n",
      "\t-wt_dedup <string>               : Walltime for post-alignment filtering (default: 23h, 24:00:00).\r\n",
      "\t-mem_dedup <string>              : Max. memory for post-alignment filtering (default: 12G).\r\n",
      "\t-dup_marker <string>             : Dup marker for filtering mapped reaads in BAMs: picard or sambamba (default: picard).\r\n",
      "\t-use_sambamba_markdup <bool>     : Use sambamba markdup instead of Picard MarkDuplicates (default: false).\r\n",
      "== postalign bed/tagalign settings\r\n",
      "\t-mem_shuf <string>               : Max. memory for UNIX shuf (default: 12G).\r\n",
      "\t-no_random_source <bool>         : Disable --random-source for UNIX shuf. Hot fix for end of file error.\r\n",
      "\t-fraglen0 <bool>                 : (LEGACY PARAM) Set predefined fragment length as zero for cross corr. analysis (add -speak=0 to run_spp.R).\r\n",
      "\t-speak_xcor <int>                : Set user-defined cross-corr. peak strandshift (-speak= in run_spp.R). Use -1 to disable (default: -1).\r\n",
      "\t-max_ppsize_xcor <string>        : R stack size (R parameter --max-ppsize=; between 5000 and 5000000) for cross corr. analysis.\r\n",
      "\t-extra_param_xcor <string>       : Set extra parameters for run_spp.R (cross-corr. analysis only).\r\n",
      "\t-mem_xcor <string>               : Max. memory for cross-corr. analysis (default: 15G).\r\n",
      "== postalign bed/tagalign settings\r\n",
      "\t-mem_shuf <string>               : Max. memory for UNIX shuf (default: 12G).\r\n",
      "\t-no_random_source <bool>         : Disable --random-source for UNIX shuf. Hot fix for end of file error.\r\n",
      "\t-fraglen0 <bool>                 : (LEGACY PARAM) Set predefined fragment length as zero for cross corr. analysis (add -speak=0 to run_spp.R).\r\n",
      "\t-speak_xcor <int>                : Set user-defined cross-corr. peak strandshift (-speak= in run_spp.R). Use -1 to disable (default: -1).\r\n",
      "\t-max_ppsize_xcor <string>        : R stack size (R parameter --max-ppsize=; between 5000 and 5000000) for cross corr. analysis.\r\n",
      "\t-extra_param_xcor <string>       : Set extra parameters for run_spp.R (cross-corr. analysis only).\r\n",
      "\t-mem_xcor <string>               : Max. memory for cross-corr. analysis (default: 15G).\r\n",
      "== callpeak macs2 settings (requirements: -chrsz -gensz)\r\n",
      "\t-gensz <string>                  : Genome size; hs for human, mm for mouse.\r\n",
      "\t-wt_macs2 <string>               : Walltime for MACS2 (default: 23h, 23:00:00).\r\n",
      "\t-mem_macs2 <string>              : Max. memory for MACS2 (default: 15G).\r\n",
      "\t-cap_num_peak_macs2 <string>     : Cap number of peaks by taking top N peaks for MACS2 (default: 300K).\r\n",
      "\t-extra_param_macs2 <string>      : Extra parameters for macs2 callpeak.\r\n",
      "== callpeak naive overlap settings\r\n",
      "\t-nonamecheck <bool>              : bedtools intersect -nonamecheck (bedtools>=2.24.0, use this if you get bedtools intersect naming convenction warnings/errors).\r\n",
      "== IDR settings\r\n",
      "\t-idr_suffix <bool>               : Append IDR threshold to IDR output directory.\r\n",
      "== ATAQC settings\r\n",
      "\t-tss_enrich <string>             : TSS enrichment bed for ataqc.\r\n",
      "\t-dnase <string>                  : DNase bed (open chromatin region file) for ataqc.\r\n",
      "\t-prom <string>                   : Promoter bed (promoter region file) for ataqc.\r\n",
      "\t-enh <string>                    : Enhancer bed (enhancer region file) for ataqc.\r\n",
      "\t-reg2map <string>                : Reg2map (file with cell type signals) for ataqc.\r\n",
      "\t-reg2map_bed <string>            : Reg2map_bed (file of regions used to generate reg2map signals) for ataqc.\r\n",
      "\t-roadmap_meta <string>           : Roadmap metadata for ataqc.\r\n",
      "\t-mem_ataqc <string>              : Max. memory for ATAQC (default: 20G).\r\n",
      "\t-wt_ataqc <string>               : Walltime for ATAQC (default: 47h, 47:00:00).\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "export PATH=$PATH:/opt/atac_dnase_pipelines\n",
    "atac.bds --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the pipeline is highly customizable and all the customizations might seem a bit confusing at first, do not worry -- for our purposes, the default settings will suffice. You will run the pipeline on your two experiments. Fill in the names of the FASTQ files corresponding to your two experiments below, as well as the name of the ouptut directory to store the processed data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "#You can find the experiment names in the file $METADATA_DIR/TC2018_samples.tsv.\n",
    "#Look under the column labeled \"ID\"\n",
    "#example: \n",
    "\n",
    "#export experiment1=\"WT_ethanol_1\"\n",
    "#export experiment2=\"asf1_ethanol_1\"\n",
    "\n",
    "export experiment1=\"WT-SCD-Rep1\"\n",
    "export experiment2=\"WT-SCD-Rep2\"\n",
    "\n",
    "#Create directories to store outputs from the pipeline\n",
    "#We will store the outputs in the $WORK_DIR\n",
    "export outdir1=$WORK_DIR/$experiment1\\_out \n",
    "export outdir2=$WORK_DIR/$experiment2\\_out\n",
    "mkdir $outdir1\n",
    "mkdir $outdir2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, kick off the pipeline! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/.bds/bds_scr atac.bds -out_dir /srv/scratch/training_camp/work/ubuntu/WT-SCD-Rep1_out -species saccer3 -fastq1_1 /srv/scratch/training_camp/work/ubuntu/data/WT-SCD-Rep1\\_R1_001.fastq.gz -fastq1_2 /srv/scratch/training_camp/work/ubuntu/data/WT-SCD-Rep1\\_R2_001.fastq.gz -nth 4\r\n",
      "[SCR_NAME] : atac.bds.BDS\r\n",
      "[HOST] : ip-172-31-26-41.us-west-1.compute.internal\r\n",
      "[LOG_FILE_NAME] : /home/ubuntu/training_camp/workflow_notebooks/atac.bds.BDS.log\r\n",
      "[BDS_PARAM] :  -out_dir /srv/scratch/training_camp/work/ubuntu/WT-SCD-Rep1_out -species saccer3 -fastq1_1 /srv/scratch/training_camp/work/ubuntu/data/WT-SCD-Rep1_R1_001.fastq.gz -fastq1_2 /srv/scratch/training_camp/work/ubuntu/data/WT-SCD-Rep1_R2_001.fastq.gz -nth 4\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "#first experiment:\n",
    "echo \"bds_scr $experiment1 $experiment1.log atac.bds -out_dir $outdir1 -species saccer3 -fastq1_1 $DATA_DIR/$experiment1\\_R1_001.fastq.gz -fastq1_2 $DATA_DIR/$experiment1\\_R2_001.fastq.gz -nth 4\"\n",
    "bds_scr $experiment1 $outdir1/log.txt atac.bds -out_dir $outdir1 -species saccer3 -fastq1_1 $DATA_DIR/$experiment1\\_R1_001.fastq.gz -fastq1_2 $DATA_DIR/$experiment1\\_R2_001.fastq.gz -nth 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bds_scr WT-SCD-Rep1 WT-SCD-Rep1.log atac.bds -out_dir /srv/scratch/training_camp/work/ubuntu/WT-SCD-Rep2_out -species saccer3 -fastq1_1 /srv/scratch/training_camp/work/ubuntu/data/.fastq.gz -fastq1_2 /srv/scratch/training_camp/work/ubuntu/data/.fastq.gz -nth 4\r\n",
      "[SCR_NAME] : WT-SCD-Rep1.BDS\r\n",
      "[HOST] : ip-172-31-26-41.us-west-1.compute.internal\r\n",
      "[LOG_FILE_NAME] : WT-SCD-Rep1.log\r\n",
      "[BDS_PARAM] :  atac.bds -out_dir /srv/scratch/training_camp/work/ubuntu/WT-SCD-Rep2_out -species saccer3 -fastq1_1 /srv/scratch/training_camp/work/ubuntu/data/WT-SCD-Rep2_R1_001.fastq.gz -fastq1_2 /srv/scratch/training_camp/work/ubuntu/data/WT-SCD-Rep2_R2_001.fastq.gz -nth 4\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "#second experiment:\n",
    "echo \"bds_scr $experiment2 $experiment2.log atac.bds -out_dir $outdir2 -species saccer3 -fastq1_1 $DATA_DIR/$experiment2_R1_001.fastq.gz -fastq1_2 $DATA_DIR/$experiment2_R2_001.fastq.gz -nth 4\"\n",
    "bds_scr $experiment2 $outdir2/log.txt atac.bds -out_dir $outdir2 -species saccer3 -fastq1_1 $DATA_DIR/$experiment2\\_R1_001.fastq.gz -fastq1_2 $DATA_DIR/$experiment2\\_R2_001.fastq.gz -nth 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline may run for an hour or so, so meanwhile, we will learn more about what it's doing under the hood. \n",
    "If you want to check on the progress, you can examine the log file generated by the pipeline:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tail: cannot open ‘/srv/scratch/training_camp/work/ubuntu/WT-SCD-Rep1_out/log.txt’ for reading: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "tail $experiment1.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tail $experiment2.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Examining the pipeline output\n",
    "\n",
    "The pipeline consists of multiple modules, with output files that include the following: \n",
    "\n",
    "```\n",
    "out                               # root dir. of outputs\n",
    "│\n",
    "├ *report.html                    #  HTML report\n",
    "├ *tracks.json                    #  Tracks datahub (JSON) for WashU browser\n",
    "├ ENCODE_summary.json             #  Metadata of all datafiles and QC results\n",
    "│\n",
    "├ align                           #  mapped alignments\n",
    "│ ├ rep1                          #   for true replicate 1 \n",
    "│ │ ├ *.trim.fastq.gz             #    adapter-trimmed fastq\n",
    "│ │ ├ *.bam                       #    raw bam\n",
    "│ │ ├ *.nodup.bam (E)             #    filtered and deduped bam\n",
    "│ │ ├ *.tagAlign.gz               #    tagAlign (bed6) generated from filtered bam\n",
    "│ │ ├ *.tn5.tagAlign.gz           #    TN5 shifted tagAlign for ATAC pipeline (not for DNase pipeline)\n",
    "│ │ └ *.*M.tagAlign.gz            #    subsampled tagAlign for cross-corr. analysis\n",
    "│ ├ rep2                          #   for true repilicate 2\n",
    "│ ...\n",
    "│ ├ pooled_rep                    #   for pooled replicate\n",
    "│ ├ pseudo_reps                   #   for self pseudo replicates\n",
    "│ │ ├ rep1                        #    for replicate 1\n",
    "│ │ │ ├ pr1                       #     for self pseudo replicate 1 of replicate 1\n",
    "│ │ │ ├ pr2                       #     for self pseudo replicate 2 of replicate 1\n",
    "│ │ ├ rep2                        #    for repilicate 2\n",
    "│ │ ...                           \n",
    "│ └ pooled_pseudo_reps            #   for pooled pseudo replicates\n",
    "│   ├ ppr1                        #    for pooled pseudo replicate 1 (rep1-pr1 + rep2-pr1 + ...)\n",
    "│   └ ppr2                        #    for pooled pseudo replicate 2 (rep1-pr2 + rep2-pr2 + ...)\n",
    "│\n",
    "├ peak                             #  peaks called\n",
    "│ └ macs2                          #   peaks generated by MACS2\n",
    "│   ├ rep1                         #    for replicate 1\n",
    "│   │ ├ *.narrowPeak.gz            #     narrowPeak (p-val threshold = 0.01)\n",
    "│   │ ├ *.filt.narrowPeak.gz (E)   #     blacklist filtered narrowPeak \n",
    "│   │ ├ *.narrowPeak.bb (E)        #     narrowPeak bigBed\n",
    "│   │ ├ *.narrowPeak.hammock.gz    #     narrowPeak track for WashU browser\n",
    "│   │ ├ *.pval0.1.narrowPeak.gz    #     narrowPeak (p-val threshold = 0.1)\n",
    "│   │ └ *.pval0.1.*K.narrowPeak.gz #     narrowPeak (p-val threshold = 0.1) with top *K peaks\n",
    "│   ├ rep2                         #    for replicate 2\n",
    "│   ...\n",
    "│   ├ pseudo_reps                          #   for self pseudo replicates\n",
    "│   ├ pooled_pseudo_reps                   #   for pooled pseudo replicates\n",
    "│   ├ overlap                              #   naive-overlapped peaks\n",
    "│   │ ├ *.naive_overlap.narrowPeak.gz      #     naive-overlapped peak\n",
    "│   │ └ *.naive_overlap.filt.narrowPeak.gz #     naive-overlapped peak after blacklist filtering\n",
    "│   └ idr                           #   IDR thresholded peaks\n",
    "│     ├ true_reps                   #    for replicate 1\n",
    "│     │ ├ *.narrowPeak.gz           #     IDR thresholded narrowPeak\n",
    "│     │ ├ *.filt.narrowPeak.gz (E)  #     IDR thresholded narrowPeak (blacklist filtered)\n",
    "│     │ └ *.12-col.bed.gz           #     IDR thresholded narrowPeak track for WashU browser\n",
    "│     ├ pseudo_reps                 #    for self pseudo replicates\n",
    "│     │ ├ rep1                      #    for replicate 1\n",
    "│     │ ...\n",
    "│     ├ optimal_set                 #    optimal IDR thresholded peaks\n",
    "│     │ └ *.filt.narrowPeak.gz (E)  #     IDR thresholded narrowPeak (blacklist filtered)\n",
    "│     ├ conservative_set            #    optimal IDR thresholded peaks\n",
    "│     │ └ *.filt.narrowPeak.gz (E)  #     IDR thresholded narrowPeak (blacklist filtered)\n",
    "│     ├ pseudo_reps                 #    for self pseudo replicates\n",
    "│     └ pooled_pseudo_reps          #    for pooled pseudo replicate\n",
    "│\n",
    "│   \n",
    "│ \n",
    "├ qc                              #  QC logs\n",
    "│ ├ *IDR_final.qc                 #   Final IDR QC\n",
    "│ ├ rep1                          #   for true replicate 1\n",
    "│ │ ├ *.align.log                 #    Bowtie2 mapping stat log\n",
    "│ │ ├ *.dup.qc                    #    Picard (or sambamba) MarkDuplicate QC log\n",
    "│ │ ├ *.pbc.qc                    #    PBC QC\n",
    "│ │ ├ *.nodup.flagstat.qc         #    Flagstat QC for filtered bam\n",
    "│ │ ├ *M.cc.qc                    #    Cross-correlation analysis score for tagAlign\n",
    "│ │ ├ *M.cc.plot.pdf/png          #    Cross-correlation analysis plot for tagAlign\n",
    "│ │ └ *_qc.html/txt               #    ATAQC report\n",
    "│ ...\n",
    "│\n",
    "├ signal                          #  signal tracks\n",
    "│ ├ macs2                         #   signal tracks generated by MACS2\n",
    "│ │ ├ rep1                        #    for true replicate 1 \n",
    "│ │ │ ├ *.pval.signal.bigwig (E)  #     signal track for p-val\n",
    "│ │ │ └ *.fc.signal.bigwig   (E)  #     signal track for fold change\n",
    "│ ...\n",
    "│ └ pooled_rep                    #   for pooled replicate\n",
    "│ \n",
    "├ report                          # files for HTML report\n",
    "└ meta                            # text files containing md5sum of output files and other metadata\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine how well the reads aligned to the reference saccer3 genome: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat $outdir1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's examine how many IDR peaks were called for each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat $outdir1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Creating a merged peak set across all samples for downstream analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we merge the peaks across all conditions to create a master list of peaks for analysis. To do this, we concatenate the IDR peaks from all experiments, sort them, and merge them. \n",
    "\n",
    "We take the output of the processing pipeline from the $AGGREGATE_ANALYSIS directory. This is the same analysis you performed above, but gathered in one location for all experiments conducted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "cd $AGGREGATE_ANALYSIS_DIR\n",
    "#Use the \"find\" command to identify all IDR narrowPeak output files and writ ethem to a file. \n",
    "\n",
    "#Now, iterate through the list of narrowPeak files and concatenate them into a single master peak list. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#concatenate all .narrowPeak files together \n",
    "cat *narrowPeak > all.peaks.bed \n",
    "\n",
    "#sort the concatenated file \n",
    "bedtools sort -i all.peaks.bed > all.peaks.sorted.bed \n",
    "\n",
    "#merge the sorted, concatenated fileto join overlapping peaks \n",
    "\n",
    "cat all.peaks.sorted.bed | awk -F '\\t' 'BEGIN {{ OFS=\"\\t\" }} {{ $2=$2+$10-100; $3=$2+$10+100; if ($2<0) {{$2 = 0}} print $0 }}' | bedtools sort | bedtools merge > all_merged.peaks.bed\n",
    "gzip -f all_merged.peaks.bed \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
