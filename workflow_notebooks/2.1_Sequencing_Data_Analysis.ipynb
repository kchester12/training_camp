{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequencing data analysis\n",
    "\n",
    "### IMPORTANT: Please make sure that your are using the bash kernel to run this notebook.\n",
    "#### (Do this at the beginning of every session) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook covers analysis of DNA sequencing data from raw files to processed signals.\n",
    "\n",
    "Although this analysis is for ATAC-seq data, many of the steps (especially the first section) are the same for other types of DNA sequencing experiments.\n",
    "\n",
    "We'll be doing the analysis in Bash, which is the standard language for UNIX command-line scripting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps in the analysis pipeline that are covered in this notebook are indicated below:\n",
    "![Sequencing Data Analysis 1](images/part1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting up the data\n",
    "\n",
    "We start with raw `.fastq.gz` files, which are provided by the sequencing instrument. For each DNA molecule (read) that was sequenced, they provide the nucleotide sequence, and information about the quality of the signal of that nucleotide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Set up variables storing the location of our data\n",
    "### The proper way to load your variables is with the ~/.bashrc command, but this is very slow in iPython \n",
    "export SUNETID=\"$(whoami)\"\n",
    "export WORK_DIR=\"/srv/scratch/training_camp/work/${SUNETID}\"\n",
    "export DATA_DIR=\"${WORK_DIR}/data\"\n",
    "[[ ! -d ${WORK_DIR}/data ]] && mkdir \"${WORK_DIR}/data\"\n",
    "export SRC_DIR=\"${WORK_DIR}/src\"\n",
    "[[ ! -d ${WORK_DIR}/src ]] && mkdir -p \"${WORK_DIR}/src\"\n",
    "export METADATA_DIR=\"/srv/scratch/training_camp/metadata\"\n",
    "export AGGREGATE_DATA_DIR=\"/srv/scratch/training_camp/data\"\n",
    "export AGGREGATE_ANALYSIS_DIR=\"/srv/scratch/training_camp/aggregate_analysis\"\n",
    "export YEAST_DIR=\"/srv/scratch/training_camp/saccer3\"\n",
    "export TMP=\"${WORK_DIR}/tmp\"\n",
    "export TEMP=$TMP\n",
    "export TMPDIR=$TMP\n",
    "[[ ! -d ${TMP} ]] && mkdir -p \"${TMP}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check exactly which fastqs we have (we copied these from \\$AGGREGATE_DATA_DIR to your personal $DATA_DIR in the last tutorial):\n",
    "\n",
    "(recall that the `ls` command lists the contents of a directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls $DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As a sanity check, we can also look at the size and last edited time of some of the fastqs by addind `-lrth` to the `ls` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls -lrth $DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also inspect the format of one of the fastqs. Notice that each read takes up 4 lines:\n",
    "1. the read name\n",
    "2. the read's nucleotide sequence\n",
    "3. a '+' to indicate the record contains another line\n",
    "4. a quality score for each base (a number encoded as a letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zcat $(ls $DATA_DIR/*gz | head -n 1) | head -n 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 2:ATAC-seq data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ENCODE consortium (https://www.encodeproject.org/) uses a standard ATAC-seq data processing pipeline, which can be downloaded here: https://github.com/ENCODE-DCC/atac-seq-pipeline\n",
    "\n",
    "This pipeline is pre-installed on this computer and can be executed by running the **atac.bds** script. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#/opt/atac_dnase_pipelines/atac.bds --help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the pipeline is highly customizable and all the customizations might seem a bit confusing at first, do not worry -- for our purposes, the default settings will suffice. You will run the pipeline on your two experiments. Fill in the names of the FASTQ files corresponding to your two experiments below, as well as the name of the ouptut directory to store the processed data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#You can find the experiment names in the file $METADATA_DIR/TC2018_samples.tsv.\n",
    "#Look under the column labeled \"ID\"\n",
    "#example: \n",
    "\n",
    "export experiment1=\"hrosenbl_WT_YPGE_1\"\n",
    "export experiment2=\"pgoddard_asf1_YPGE_1\"\n",
    "\n",
    "#Create directories to store outputs from the pipeline\n",
    "\n",
    "#We will store the outputs in the $WORK_DIR\n",
    "export outdir1=$WORK_DIR/$experiment1\\_out \n",
    "export outdir2=$WORK_DIR/$experiment2\\_out\n",
    "mkdir $outdir1\n",
    "mkdir $outdir2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, kick off the pipeline! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#first experiment:\n",
    "echo \"bds_scr $experiment1 $experiment1.log atac.bds -out_dir $outdir1 -species saccer3 -fastq1_1 $DATA_DIR/$experiment1\\_R1_001.fastq.gz -fastq1_2 $DATA_DIR/$experiment1\\_R2_001.fastq.gz -nth 4\"\n",
    "bds_scr $experiment1 $outdir1/log.txt /opt/atac_dnase_pipelines/atac.bds -out_dir $outdir1 -species saccer3 -fastq1_1 $DATA_DIR/$experiment1\\_R1_001.fastq.gz -fastq1_2 $DATA_DIR/$experiment1\\_R2_001.fastq.gz -nth 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#second experiment:\n",
    "echo \"bds_scr $experiment2 $experiment2.log atac.bds -out_dir $outdir2 -species saccer3 -fastq1_1 $DATA_DIR/$experiment2_R1_001.fastq.gz -fastq1_2 $DATA_DIR/$experiment2_R2_001.fastq.gz -nth 4\"\n",
    "bds_scr $experiment2 $outdir2/log.txt /opt/atac_dnase_pipelines/atac.bds -out_dir $outdir2 -species saccer3 -fastq1_1 $DATA_DIR/$experiment2\\_R1_001.fastq.gz -fastq1_2 $DATA_DIR/$experiment2\\_R2_001.fastq.gz -nth 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline may run for an hour or so, so meanwhile, we will learn more about what it's doing under the hood. \n",
    "If you want to check on the progress, you can examine the latest entries in the  log file generated by the pipeline with teh *tail* command. The log files are specified by the [LOG_FILE_NAME] entry above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tail $outdir1/log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tail  $outdir2/log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Examining the pipeline output\n",
    "\n",
    "The pipeline consists of multiple modules, with output files that include the following: \n",
    "\n",
    "```\n",
    "out                               # root dir. of outputs\n",
    "│\n",
    "├ *report.html                    #  HTML report\n",
    "├ *tracks.json                    #  Tracks datahub (JSON) for WashU browser\n",
    "├ ENCODE_summary.json             #  Metadata of all datafiles and QC results\n",
    "│\n",
    "├ align                           #  mapped alignments\n",
    "│ ├ rep1                          #   for true replicate 1 \n",
    "│ │ ├ *.trim.fastq.gz             #    adapter-trimmed fastq\n",
    "│ │ ├ *.bam                       #    raw bam\n",
    "│ │ ├ *.nodup.bam (E)             #    filtered and deduped bam\n",
    "│ │ ├ *.tagAlign.gz               #    tagAlign (bed6) generated from filtered bam\n",
    "│ │ ├ *.tn5.tagAlign.gz           #    TN5 shifted tagAlign for ATAC pipeline (not for DNase pipeline)\n",
    "│ │ └ *.*M.tagAlign.gz            #    subsampled tagAlign for cross-corr. analysis\n",
    "│ ├ rep2                          #   for true repilicate 2\n",
    "│ ...\n",
    "│ ├ pooled_rep                    #   for pooled replicate\n",
    "│ ├ pseudo_reps                   #   for self pseudo replicates\n",
    "│ │ ├ rep1                        #    for replicate 1\n",
    "│ │ │ ├ pr1                       #     for self pseudo replicate 1 of replicate 1\n",
    "│ │ │ ├ pr2                       #     for self pseudo replicate 2 of replicate 1\n",
    "│ │ ├ rep2                        #    for repilicate 2\n",
    "│ │ ...                           \n",
    "│ └ pooled_pseudo_reps            #   for pooled pseudo replicates\n",
    "│   ├ ppr1                        #    for pooled pseudo replicate 1 (rep1-pr1 + rep2-pr1 + ...)\n",
    "│   └ ppr2                        #    for pooled pseudo replicate 2 (rep1-pr2 + rep2-pr2 + ...)\n",
    "│\n",
    "├ peak                             #  peaks called\n",
    "│ └ macs2                          #   peaks generated by MACS2\n",
    "│   ├ rep1                         #    for replicate 1\n",
    "│   │ ├ *.narrowPeak.gz            #     narrowPeak (p-val threshold = 0.01)\n",
    "│   │ ├ *.filt.narrowPeak.gz (E)   #     blacklist filtered narrowPeak \n",
    "│   │ ├ *.narrowPeak.bb (E)        #     narrowPeak bigBed\n",
    "│   │ ├ *.narrowPeak.hammock.gz    #     narrowPeak track for WashU browser\n",
    "│   │ ├ *.pval0.1.narrowPeak.gz    #     narrowPeak (p-val threshold = 0.1)\n",
    "│   │ └ *.pval0.1.*K.narrowPeak.gz #     narrowPeak (p-val threshold = 0.1) with top *K peaks\n",
    "│   ├ rep2                         #    for replicate 2\n",
    "│   ...\n",
    "│   ├ pseudo_reps                          #   for self pseudo replicates\n",
    "│   ├ pooled_pseudo_reps                   #   for pooled pseudo replicates\n",
    "│   ├ overlap                              #   naive-overlapped peaks\n",
    "│   │ ├ *.naive_overlap.narrowPeak.gz      #     naive-overlapped peak\n",
    "│   │ └ *.naive_overlap.filt.narrowPeak.gz #     naive-overlapped peak after blacklist filtering\n",
    "│   └ idr                           #   IDR thresholded peaks\n",
    "│     ├ true_reps                   #    for replicate 1\n",
    "│     │ ├ *.narrowPeak.gz           #     IDR thresholded narrowPeak\n",
    "│     │ ├ *.filt.narrowPeak.gz (E)  #     IDR thresholded narrowPeak (blacklist filtered)\n",
    "│     │ └ *.12-col.bed.gz           #     IDR thresholded narrowPeak track for WashU browser\n",
    "│     ├ pseudo_reps                 #    for self pseudo replicates\n",
    "│     │ ├ rep1                      #    for replicate 1\n",
    "│     │ ...\n",
    "│     ├ optimal_set                 #    optimal IDR thresholded peaks\n",
    "│     │ └ *.filt.narrowPeak.gz (E)  #     IDR thresholded narrowPeak (blacklist filtered)\n",
    "│     ├ conservative_set            #    optimal IDR thresholded peaks\n",
    "│     │ └ *.filt.narrowPeak.gz (E)  #     IDR thresholded narrowPeak (blacklist filtered)\n",
    "│     ├ pseudo_reps                 #    for self pseudo replicates\n",
    "│     └ pooled_pseudo_reps          #    for pooled pseudo replicate\n",
    "│\n",
    "│   \n",
    "│ \n",
    "├ qc                              #  QC logs\n",
    "│ ├ *IDR_final.qc                 #   Final IDR QC\n",
    "│ ├ rep1                          #   for true replicate 1\n",
    "│ │ ├ *.align.log                 #    Bowtie2 mapping stat log\n",
    "│ │ ├ *.dup.qc                    #    Picard (or sambamba) MarkDuplicate QC log\n",
    "│ │ ├ *.pbc.qc                    #    PBC QC\n",
    "│ │ ├ *.nodup.flagstat.qc         #    Flagstat QC for filtered bam\n",
    "│ │ ├ *M.cc.qc                    #    Cross-correlation analysis score for tagAlign\n",
    "│ │ ├ *M.cc.plot.pdf/png          #    Cross-correlation analysis plot for tagAlign\n",
    "│ │ └ *_qc.html/txt               #    ATAQC report\n",
    "│ ...\n",
    "│\n",
    "├ signal                          #  signal tracks\n",
    "│ ├ macs2                         #   signal tracks generated by MACS2\n",
    "│ │ ├ rep1                        #    for true replicate 1 \n",
    "│ │ │ ├ *.pval.signal.bigwig (E)  #     signal track for p-val\n",
    "│ │ │ └ *.fc.signal.bigwig   (E)  #     signal track for fold change\n",
    "│ ...\n",
    "│ └ pooled_rep                    #   for pooled replicate\n",
    "│ \n",
    "├ report                          # files for HTML report\n",
    "└ meta                            # text files containing md5sum of output files and other metadata\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine how well the reads aligned to the reference saccer3 genome. We'd like to see an overall alignment rate >=90% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat $outdir1/qc/rep1/*align.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat $outdir2/qc/rep1/*align.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's examine how many  peaks were called for each sample. We use the *zcat* command to examine the contents of a zipped file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zcat $outdir1/peak/macs2/overlap/optimal_set/*narrowPeak.gz | wc -l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zcat $outdir2/peak/macs2/overlap/optimal_set/*narrowPeak.gz | wc -l "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing signal tracks in the WashU and UCSC genome browsers ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline uses the MACS2 peak caller to generate two types of signal tracks across the yeast genome: \n",
    "\n",
    "* P-value Tracks \n",
    "* Fold Change Tracks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls $outdir1/signal/macs2/rep1/\n",
    "\n",
    "ls $outdir2/signal/macs2/rep1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files are in binary format, so we cannot print their contents to the terminal, but a number of genome browser tools have been developed that allow us to visualize their contents.  Two of the most popular of these are\n",
    "\n",
    "* UCSC Genome Browser (https://genome.ucsc.edu/cgi-bin/hgGateway) \n",
    "\n",
    "* WashU Epigenome Browser (https://epigenomegateway.wustl.edu/) \n",
    "\n",
    "Both browsers enable you to upload or link your data for visualization. The most efficient way to do this, is to place your bigwig files on a publically accessible  web server, and to link to them from the browser. \n",
    "\n",
    "We have uploaded the fold change and pval bigwigs to the mitra server, here: \n",
    "\n",
    "mitra.stanford.edu/kundaje/tc2018\n",
    "\n",
    "In that directory, you see a folder of fc bigwig tracks (http://mitra.stanford.edu/kundaje/tc2018/fc_tracks/) as well as a folder of pval bigwig tracks (http://mitra.stanford.edu/kundaje/tc2018/pvalue_tracks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the full set of fc or pval bigwigs by following this link: http://mitra.stanford.edu/kundaje/tc2018/saccer3_tracks.html \n",
    "\n",
    "We will now go step-by-step through the process used to generate this visualization. To begin, point your browser to \n",
    "https://epigenomegateway.wustl.edu/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's quite inefficient to upload our 35 track files one by one. To visualiza files in bulk, the WashU browser allows you to upload \"datahubs\". A datahub is  a file in the json format, which use a nested syntax to specify attributes about how the files are to be visualized. If you're curious, there's more information about such json \"datahubs\" here: http://washugb.blogspot.com/2012/04/data-hub.html. \n",
    "\n",
    "\n",
    "We have generated datahubs for our fc and pval bigwig files here: \n",
    "\n",
    "http://mitra.stanford.edu/kundaje/tc2018/pval.datahub.json and\n",
    "\n",
    "http://mitra.stanford.edu/kundaje/tc2018/fc.datahub.json\n",
    "\n",
    "don't worry about the syntax of these files for now (you can generally copy the syntax of these and just replace your file names and urls). The main point is to be aware that these hubs can be used to group visualizations of multiple browser tracks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Creating a merged peak set across all samples for downstream analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we merge the peaks across all conditions to create a master list of peaks for analysis. To do this, we concatenate the IDR peaks from all experiments, sort them, and merge them. \n",
    "\n",
    "We take the output of the processing pipeline from the $AGGREGATE_ANALYSIS directory. This is the same analysis you performed above, but gathered in one location for all experiments conducted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cd $WORK_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Use the \"find\" command to identify all IDR narrowPeak output files and write them to a file. \n",
    "find -L $AGGREGATE_ANALYSIS_DIR  -wholename \"*peak/macs2/overlap/optimal_set/*narrowPeak.gz\" > narrowPeak_files.txt\n",
    "\n",
    "#sanity check the file \n",
    "head narrowPeak_files.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now, iterate through the list of narrowPeak files and concatenate them into a single master peak list. \n",
    "for f in `cat narrowPeak_files.txt`\n",
    "do \n",
    "    zcat $f >> all.peaks.bed\n",
    "done\n",
    "\n",
    "#sanity check the all.peaks.bed file \n",
    "head all.peaks.bed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sort the concatenated file \n",
    "bedtools sort -i all.peaks.bed > all.peaks.sorted.bed \n",
    "\n",
    "head all.peaks.sorted.bed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#merge the sorted, concatenated fileto join overlapping peaks \n",
    "bedtools merge -i all.peaks.sorted.bed > all_merged.peaks.bed \n",
    "\n",
    "head all_merged.peaks.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Finally, we use the awk command to add row numbers to the merged peak file, such that each peak has a unique identifier. \n",
    "\n",
    "#We cannot do this 'in place', so we use an intermediate output file \n",
    "awk  -v OFS='\\t' '{print $0,NR}' all_merged.peaks.bed > o.tmp\n",
    "mv o.tmp all_merged.peaks.bed\n",
    "\n",
    "head all_merged.peaks.bed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Creating read count and fold change matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to calculate the signal strength in each sample at the genomic regions in **all_merged.peaks.bed**. As we saw above, the ATAC-seq pipeline generates genome-wide fold change signal tracks for each sample that can be used for this calculation (the \\*fc.bigwig and \\*pval.bigwig files). We use the **bigWigAverageOverBed** utility to computue the mean signal from the pval tracks and the mean signal from the fold change tracks for each genomic region in each sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigWigAverageOverBed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First, we find all the fold change bigWig files\n",
    "cd $WORK_DIR\n",
    "find -L $AGGREGATE_ANALYSIS_DIR  -name \"*fc*bigwig\" > all.fc.bigwig\n",
    "head all.fc.bigwig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wc -l all.fc.bigwig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Iterate through all bigWig fold change tracks to compute mean signal strength at each genomic region \n",
    "for f in `cat all.fc.bigwig`\n",
    "do\n",
    "\n",
    "    #we extract the part of the filename that corresponds to the sample name and write it as the header in the fc.signal file\n",
    "    sample_name=`basename $f | awk -F'[.]' '{print $1}'`\n",
    "    echo \"$sample_name\"\n",
    "    echo $sample_name > $sample_name.fc.signal.tmp \n",
    "    \n",
    "    \n",
    "    bigWigAverageOverBed $f all_merged.peaks.bed $sample_name.fc.signal.data.tmp \n",
    "    cut -f5 $sample_name.fc.signal.data.tmp >> $sample_name.fc.signal.tmp\n",
    "\n",
    "    #cleanup the intermediate file \n",
    "    rm $sample_name.fc.signal.data.tmp \n",
    "done\n",
    "paste *fc.signal.tmp > all.fc.txt\n",
    "#cleanup intermediate files that were generated \n",
    "rm *.tmp\n",
    "\n",
    "#examine the output \n",
    "head all.fc.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the fold change data matrix, we would also like to know the number of reads that pile up at each peak region. This is useful for determining differential chromatin accessibility across samples. \n",
    "To calculate the read count matrix, we will use the **bedtools coverage** command on the *tagAlign* files generated by the processing pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First, we find all the tagAlign\n",
    "cd $WORK_DIR\n",
    "find -L $AGGREGATE_ANALYSIS_DIR  -name \"*nodup.tn5.no_chrM.25M.R1.tagAlign*\" > all.tagAlign.files.txt\n",
    "\n",
    "head all.tagAlign.files.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wc -l all.tagAlign.files.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's see how the bedtools coverage command works\n",
    "bedtools coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Iterate through all tagAlign files to compute read count at each peak region.  \n",
    "for f in `cat all.tagAlign.files.txt`\n",
    "do\n",
    "    sample_name=`basename $f | awk -F'[.]' '{print $1}'`\n",
    "    echo \"$sample_name\"\n",
    "    echo $sample_name > $sample_name.readcount.tmp \n",
    "    zcat $f | bedtools coverage -counts -a stdin -b all_merged.peaks.bed  | cut -f5 >>$sample_name.readcount.tmp \n",
    "done\n",
    "paste *.readcount.tmp > all.readcount.txt\n",
    "#cleanup the temporary files\n",
    "rm *.tmp\n",
    "\n",
    "#examine the output \n",
    "head all.readcount.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the counts in the first and second columns are on a different scale. This makes sense because if a particular sample had more reads to begin with, the raw counts for each peak will be higher. \n",
    "We can address this problem with sample normalization, covered in the next section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Finally, we add in the peak names to our counts file and fold change file so we can keep track of which row \n",
    "#corresponds to which peak. \n",
    "\n",
    "\n",
    "#add a header to the merged peak file \n",
    "sed -i '1i\\Chrom\\tStart\\tEnd\\tID' all_merged.peaks.bed\n",
    "\n",
    "#paste the peak bed file region annotation matrix to the signal matrix\n",
    "paste all_merged.peaks.bed all.fc.txt > o.tmp \n",
    "mv o.tmp all.fc.txt \n",
    "\n",
    "paste all_merged.peaks.bed all.readcount.txt > o.tmp\n",
    "mv o.tmp all.readcount.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head all.readcount.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head all.fc.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In examining the files, we notice that all the files end with the suffix \"\\_R1_001\". This is an artifact generated by the processing pipeline. This part of the filename is not informative for our purposes, since it's shared by all samples, so we can remove it with the **sed** command. The syntax is illustrated below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sed -i 's/_R1_001//g' all.fc.txt\n",
    "sed -i 's/_R1_001//g' all.readcount.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head all.fc.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head all.readcount.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now generated a read count matrix and a fold change signal peak regions in our dataset. \n",
    "This completes the basic data processing pipeline. \n",
    "Now, on to drawing conclusions about our data. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
